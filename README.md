# FLAN-T5 Finetuning

## Overview
This repository demonstrates the process of fine-tuning the FLAN-T5 model from Hugging Face for dialogue summarization tasks and evaluating it using ROUGE metrics. FLAN-T5 is a powerful text-to-text transfer transformer that can be adapted for a variety of NLP tasks.

---

## Features
- Fine-tuning FLAN-T5 on custom datasets
- Evaluation using ROUGE metrics
- Easy-to-follow structure for efficient experimentation

---

## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/CodeX-Addy/Fine-Tuning-FLAN-T5.git
   cd Fine-Tuning-FLAN-T5
   ```

---

## Dependencies
- Python 3.8+
- Transformers library
- Datasets library
- ROUGE score library

Install dependencies using:
```bash
pip install transformers datasets scikit-learn rouge-score
```

---

## Results
After fine-tuning, you can expect to see improvements in ROUGE metrics such as:
- **ROUGE-1**: Recall, Precision, F1-Score
- **ROUGE-2**: Recall, Precision, F1-Score
- **ROUGE-L**: Recall, Precision, F1-Score


## Contributing
Contributions are welcome! Please follow these steps:
1. Fork the repository
2. Create a new branch (`git checkout -b feature-branch`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature-branch`)
5. Open a pull request


---

## License
This project is licensed under the BSD Clause License. See the `LICENSE` file for details.

---

## Acknowledgments
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [ROUGE Metric](https://pypi.org/project/rouge-score/)

---

## Contact
For questions or suggestions, feel free to reach out:
- Email: aditya.tomar9055049@gmail.com

